---
title: "Super Learning Done Right (working title)"
author: "Jeremy Coyle & Nima Hejazi"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
bibliography: intro_sl3_refs.bib
vignette: >
  %\VignetteIndexEntry{Super Learning Done Right}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, echo=FALSE, results='hide'}
library(sl3)
```
## Introduction

The `sl3` package provides a modern framework for Machine Learning. This includes the Super Learner
algorithm [@vdl2007super], a method for performing stacked regressions
[@breiman1996stacked] and combining this with covariate screening and
cross-validation. `sl3` uses an Object Oriented Programming (OOP) approach and leverages [`R6`](https://cran.r-project.org/web/packages/R6/vignettes/Introduction.html)  classes to define both _Tasks_ (machine learning problems) and _Learners_ (machine learning algorithms that solve those problems) in a way that is flexible and extensible. The design of `sl3` owes a lot to the `SuperLearner` and `mlr` packages that also provide a unified framework for machine learning.

### Example Data

Throughout this vignette, we use data from the Collaborative Perinatal Project (CPP) to illustrate `sl3` features and usage. 

We use several standard packages (e.g., `dplyr`) and a data set included with
the `sl3` package to begin looking at how we can perform covariate screening,
model stacking, model cross-validation, and combining each of these to invoke
the Super Learner algorithm.

```{r prelims}
set.seed(49753)
library(data.table)
library(dplyr)

# packages we'll be using
library(sl3)
library(origami)
library(SuperLearner)

# load example data set
data(cpp)
cpp <- cpp %>%
  dplyr::filter(!is.na(haz)) %>%
  mutate_all(funs(replace(., is.na(.), 0)))

# here are the covariates we are interested in, and the outcome of course
covars <- c("apgar1", "apgar5", "parity", "gagebrth", "mage", "meducyrs",
            "sexn")
outcome <- "haz"
```


In the above, after setting up our analysis environment, we simply modified the
included data set via a very simple imputation rule: all rows for which the
outcome of interest has a missing value of `NA` have all `NA` values replaced by
zero. We then simply create character vectors to indicate the covariates of
interest and the outcome.

### Object Oriented Programming Basics

As mentioned above, `sl3` is designed using OOP principles. While we've tried to make it easy to use `sl3` without a deep understanding of OOP, it is helpful to have some intuition about how `sl3` is structured. In this section, we briefly outline some key concepts from OOP. Readers familiar with OOP basics can skip this section. The key concept of OOP is that of an _object_, a collection of data and functions that corresponds to some conceptual unit. Objects have two main types of elements, _fields_, which can be thought of as nouns, are information about an object, and _methods_, which can be thought of as verbs, are things an object can do. Objects are members of _classes_ that define what these fields and methods are. Classes can _inherit_ elements from other classes (sometimes called _base classes_), so that classes that are similar, but not exactly the same, can share some parts of their definitions. 

Many different implementations of OOP exist which vary in how these concepts are implemented and used. R  has several different implementations, including S3, S4, reference classes, and R6. `sl3` uses the [`R6`](https://cran.r-project.org/web/packages/R6/vignettes/Introduction.html) implementation. In R6, methods and fields of a class object are accessed using the `$` operator. The next section explains how these concepts are used in `sl3`


## `sl3` objects

`sl3` defines two key classes: `sl3_Task`, and `Lrnr_base`, that define machine learning problems and machine learning algorithms respectively. In the next sections, we describe these objects and how they are used to fit machine learning algorithms and generate prediction

### `sl3_Task`
`sl3_Task` defines a machine learning problem. 

The 
We can see an example of that here, using the cpp dataset described above:


Now that we just set up our problem, we are ready to begin looking at how `sl3`
might be used. First, we will need to define a "task" object, with a class of
`sl3_Task` (based on the R6 class system).

```{r sl3-task-create}
task <- sl3_Task$new(cpp, covariates = covars, outcome = outcome)
```

Let's take a look at this object:

```{r sl3-task-examine}
task
```

The object is composed of several nodes, defining the outcome and covariates, as
well as other information that might be useful (e.g., weights to assign to
specific covariates, and subject IDs to effectively deal with repeated
measures).

```{r sl3-task-nodes}
task$nodes
```



`sl3_Task` objects have a range of fields and methods that facillitate fitting machine learning algorithms to the underlying data. However, these are mostly used internally by learners, and can be ignored by the user. These features are documented in the help for [`sl3_Task`](../help/sl3_Task).


## Learners

`sl3` defines Learners as classes that inherit from the `Lrnr_base` base class.


`Lrnr_base` is a base class for defining a machine learning algorithm, and also a _fit_ of that algorithm to a particular task. `Lrnr_base` defines three main methods: `Lrnr_base$train`, `Lrnr_base$predict`, and `Lrnr_base$chain`.

Different machine learning algorithms are defined in classes that inherit for `Lrnr_base`. For instance, the `Lrnr_glm` class defines a learner that fits generalized linear models.

### Finding Learners
Learners have _properties_ that indicate what features they support. You can use `sl3_list_properties` to get a list of all properties supported by at least one learner. You can then use `sl3_list_learners` to find learners supporting any set of properties. For example:

```{r sl3-list-learner}
sl3_list_properties()

sl3_list_learners(c("binomial", "offset"))
```

The list of supported learners is currently rather limited. Some learners not yet supported natively in `sl3` can be used via their `SuperLearner` wrappers. `SuperLearner` wrappers, screeners, and methods can all be used as `sl3` learners via `Lrnr_pkg_SuperLearner`, `Lrnr_pkg_SuperLearner_screener`, and `Lrnr_pkg_SuperLearner_method` respectively. Here's an example

```{r SuperLearner Wrapper}

#TODO 
```

In most cases, using these wrappers will not be as efficient as their native `sl3` counterparts. If your favorite learner is missing from `sl3`, please consider adding it by following the [Defining New Learners vignette](custom_lrnrs.html).

### Learner Parameters

In general, learners can be instantiated without providing any additional parameters. We've tried to provide sensible defaults for each learner. However, if you would like to modify the learners' behavior, you can do so by instantiating learners with different parameters.

`sl3` Learners support some common parameters that work with all learners for which they are applicable:

* `covariates`: subsets covariates before fitting. This allows different learners to be fit to the same task with different covariate subsets.
* `outcome_type`: overrides the `task$outcome_type`. This allows different learners to be fit to the same task with different outcome_types.
* `transform_offset`: for learners that use offsets, this indicates that the `offset` should be transformed before being passed to the learner. 
* `...`: abitrary parameters typically passed directly to the internal learner method. The documentation for each learner will direct to the appropriate function documentation for the learner method.

## Composing Learners
<a name="composition"></a>

`sl3` defines two special learners, `Pipeline` and `Stack`, that allow learners to be composed in a flexible manner.





As discussed above, the design of `sl3` allows for screening and learning
algorithms to be _chained_ to form _pipelines_. Let's take a look at how we
might set up a pipeline.

To begin, we'll need to specify a screening algorithm to use in covariate
selection as well as a learning algorithm. We set up a simple elastic net
screener (accessed directly from the wide selection already available in the
`Super Learner` package) and a GLM learner, provided directly with the `sl3`
package.

```{r sl3-learners-screeners}
slscreener <- Lrnr_pkg_SuperLearner_screener$new("screen.glmnet")
glm_learner <- Lrnr_glm$new()
```

Having specified a screener and a learner, we are now ready to set up a
pipeline, combining these algorithms in the manner outlined by the Super Learner
algorithm.

```{r sl3-pipelines}
screen_and_glm <- Pipeline$new(slscreener, glm_learner)
SL.glmnet_learner <- Lrnr_pkg_SuperLearner$new(SL_wrapper = "SL.glmnet")
sg_fit <- screen_and_glm$train(task)
print(sg_fit)
```

The output from the `Pipeline` object provides us with a few key pieces of
information:

1. The screening algorithm (`glmnet` in this case), or library of screeners, as
   well as the variables selected by the screener.
2. The learning algorithm (`glm` in this case), or library of learners, as well
   as the standard output produced by the learning algorithm being invoked, when
   fit with the covariates selected by the screener.

What happens if we want to perform model stacking (whether with discrete or
ensemble Super Learner)?

```{r sl3-stack}
learner_stack <- Stack$new(SL.glmnet_learner, glm_learner, screen_and_glm)
stack_fit <- learner_stack$train(task)
```

As is clear from the above, we can create a _stack_ of learners (of a class
named eponymously, and sub-classing the `Lrnr_base` class) with a rather simple
call; moreover, stacks may include not only learners themselves but also
pipelines. (Recall that we created a pipeline above by combining both a screener
and a learner). Notably, since cross-validation is applied as part of model
stacking in the Super Learner algorithm, such a stacking design means that
pipelines themselves may be cross-validated. What is more, from the calls
immediately above, we note that creating stacked regressions and training such
models is an essentially trivial process.

We can examine the results of our stacked regression model by looking at the
predictions (made on the training data, though we could very easily pass in new
data as well):

```{r sl3-stack-preds}
preds <- stack_fit$predict()
head(preds)
```

In the above, we obtain prediction for the first few observations in the data
set, for each of the learners that compose the stacked regression model.

We can create a stacked regression model that incorporates cross-validation by
simply using the built-in `Lrnr_cv` class, and we can train the
cross-validated stacked regression model using its `train` method:

```{r sl3-cv-stack}
cv_stack <- Lrnr_cv$new(learner_stack)
cv_fit <- cv_stack$train(task)
```

Note that in the above we create our cross-validated stacked regression model by
invoking a new `Lrnr_cv` and passing in the model stack that we built above.
This means that our `Lrnr_cv` is built on top of our previous call to
`Stack$new()`.

To fit a meta-learner on the cross-validated predictions, we need only create a
new pipeline -- one that includes the appropriate model stack and specifies the
meta-learning algorithm to be used. Here, we create a meta-learner from a GLM
(by specifying `glm_learner`) and provide the cross-validated stacked regression
model as the library over which the meta-learner is to operate.

```{r sl3-metalearner-glm}
glm_stack <- Pipeline$new(cv_stack, glm_learner)
ml_fit <- glm_stack$train(task)
```

Above, we build the meta-learner by using a pipeline and then simply call the
`train` method from the meta-learner, passing in the data object (the `Task` we
created at the very beginning of this exercise).

Let us take a look at the object created from training the GLM meta-learner:

```{r sl3-mlfit-mod}
print(ml_fit)
```

And the first few prediction from the model:

```{r sl3-mlfit-preds}
ml_fit_preds <- ml_fit$predict()
head(ml_fit_preds)
```

Finally, we can build a proper Super Learner in a single call by providing the
library of learners as well as the meta-learner. For simplicity, we will use the
same set of learners and meta-learning algorithm as we did before, which means
that we are merely building a GLM-based Super Learner (of class `Lrnr_sl`), with
learning algorithms including an elastic net, GLM, and GLM with elastic net
screener. Below, we train the Super Learner on the data simply by invoking the
`train` method:

```{r sl3-learner-SL}
# convenience learner combining all this
sl <- Lrnr_sl$new(learners = list(SL.glmnet_learner, glm_learner,
                                  screen_and_glm),
                  metalearner = glm_learner)
sl_fit <- sl$train(task)
```

The `Lrnr_sl` object we create includes a great deal of information (thus we
regrain from examining it in detail).

## `delayed` Computation


---

## Session Information

```{r sessionInfo, echo=FALSE}
sessionInfo()
```

---

## References

