<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Super Learning Done Right (working title) • sl3</title>
<!-- jquery --><script src="https://code.jquery.com/jquery-3.1.0.min.js" integrity="sha384-nrOSfDHtoPMzJHjVTdCopGqIqeYETSXhZDFyniQ8ZHcVy08QesyHcnOUpMpqnmWq" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script><!-- Font Awesome icons --><link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous">
<!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../jquery.sticky-kit.min.js"></script><script src="../pkgdown.js"></script><!-- mathjax --><script src="https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="container template-vignette">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="../index.html">sl3</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="..//index.html">
    <span class="fa fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/custom_lrnrs.html">Defining New Learners</a>
    </li>
    <li>
      <a href="../articles/intro_sl3.html">Super Learning Done Right (working title)</a>
    </li>
  </ul>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/jeremyrcoyle/sl3">
    <span class="fa fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      
      </header><div class="row">
  <div class="col-md-9">
    <div class="page-header toc-ignore">
      <h1>Super Learning Done Right (working title)</h1>
                        <h4 class="author">Jeremy Coyle &amp; Nima Hejazi</h4>
            
            <h4 class="date">2017-10-09</h4>
          </div>

    
    
<div class="contents">
<div id="introduction" class="section level2">
<h2 class="hasAnchor">
<a href="#introduction" class="anchor"></a>Introduction</h2>
<p>The <code>sl3</code> package provides a modern implementation of the Super Learner algorithm <span class="citation">(van der Laan, Polley, and Hubbard 2007)</span>, a method for performing stacked regressions <span class="citation">(Breiman 1996)</span> and combining this with covariate screening and cross-validation. Several key design principles make the <code>sl3</code> implementation…</p>
<!--
Nima: to be expanded, not sure how much detail to provide yet...
-->
<p>The advantages that <code>sl3</code> provides are perhaps best illustrated by example.</p>
<hr>
</div>
<div id="a-whirlwind-tour" class="section level2">
<h2 class="hasAnchor">
<a href="#a-whirlwind-tour" class="anchor"></a>A Whirlwind Tour</h2>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="cf">if</span> (<span class="op">!</span>(<span class="st">"sl3"</span> <span class="op">%in%</span><span class="st"> </span><span class="kw">installed.packages</span>())) {
  devtools<span class="op">::</span><span class="kw"><a href="http://www.rdocumentation.org/packages/devtools/topics/install_github">install_github</a></span>(<span class="st">"jeremyrcoyle/sl3"</span>)
}</code></pre></div>
<p>We use several standard packages (e.g., <code>dplyr</code>) and a data set included with the <code>sl3</code> package to begin looking at how we can perform covariate screening, model stacking, model cross-validation, and combining each of these to invoke the Super Learner algorithm.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">49753</span>)
<span class="kw">library</span>(data.table)
<span class="kw">library</span>(dplyr)</code></pre></div>
<pre><code>## 
## Attaching package: 'dplyr'</code></pre>
<pre><code>## The following objects are masked from 'package:data.table':
## 
##     between, first, last</code></pre>
<pre><code>## The following objects are masked from 'package:stats':
## 
##     filter, lag</code></pre>
<pre><code>## The following objects are masked from 'package:base':
## 
##     intersect, setdiff, setequal, union</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># packages we'll be using</span>
<span class="kw">library</span>(sl3)
<span class="kw">library</span>(origami)</code></pre></div>
<pre><code>## origami: Generalized Cross-Validation Framework</code></pre>
<pre><code>## Version: 0.8.0</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(SuperLearner)</code></pre></div>
<pre><code>## Loading required package: nnls</code></pre>
<pre><code>## Super Learner</code></pre>
<pre><code>## Version: 2.0-23-9000</code></pre>
<pre><code>## Package created on 2017-07-20</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># load example data set</span>
<span class="kw">data</span>(cpp)
cpp &lt;-<span class="st"> </span>cpp <span class="op">%&gt;%</span>
<span class="st">  </span>dplyr<span class="op">::</span><span class="kw"><a href="http://dplyr.tidyverse.org/reference/filter.html">filter</a></span>(<span class="op">!</span><span class="kw">is.na</span>(haz)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw"><a href="http://dplyr.tidyverse.org/reference/summarise_all.html">mutate_all</a></span>(<span class="kw"><a href="http://dplyr.tidyverse.org/reference/funs.html">funs</a></span>(<span class="kw">replace</span>(., <span class="kw">is.na</span>(.), <span class="dv">0</span>)))

<span class="co"># here are the covariates we are interested in, and the outcome of course</span>
covars &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">"apgar1"</span>, <span class="st">"apgar5"</span>, <span class="st">"parity"</span>, <span class="st">"gagebrth"</span>, <span class="st">"mage"</span>, <span class="st">"meducyrs"</span>,
            <span class="st">"sexn"</span>)
outcome &lt;-<span class="st"> "haz"</span></code></pre></div>
<p>In the above, after setting up our analysis environment, we simply modified the included data set via a very simple imputation rule: all rows for which the outcome of interest has a missing value of <code>NA</code> have all <code>NA</code> values replaced by zero. We then simply create character vectors to indicate the covariates of interest and the outcome.</p>
<p>Now that we just set up our problem, we are ready to begin looking at how <code>sl3</code> might be used. First, we will need to define a “task” object, with a class of <code>sl3_Task</code> (based on the R6 class system).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">task &lt;-<span class="st"> </span>sl3_Task<span class="op">$</span><span class="kw">new</span>(cpp, <span class="dt">covariates =</span> covars, <span class="dt">outcome =</span> outcome)</code></pre></div>
<p>Let’s take a look at this object:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">task</code></pre></div>
<pre><code>## &lt;sl3_Task&gt;
##   Public:
##     add_columns: function (fit_uuid, new_data, global_cols = FALSE) 
##     add_interactions: function (interactions) 
##     clone: function (deep = FALSE) 
##     column_names: active binding
##     data: active binding
##     folds: active binding
##     get_data: function (rows = NULL, columns) 
##     get_node: function (node_name, generator_fun = NULL) 
##     id: active binding
##     initialize: function (data, covariates, outcome, outcome_type = NULL, id = NULL, 
##     next_in_chain: function (covariates = NULL, outcome = NULL, id = NULL, weights = NULL, 
##     nodes: active binding
##     nrow: active binding
##     outcome_type: NULL
##     subset_task: function (row_index) 
##     uuid: active binding
##     weights: active binding
##     X: active binding
##     X_intercept: active binding
##     Y: active binding
##   Private:
##     .column_names: list
##     .data: data.table, data.frame
##     .folds: list
##     .nodes: list
##     .row_index: NULL
##     .uuid: 2d0d236e-ad20-11e7-a0c3-a8667f0792e4
##     .X: NULL</code></pre>
<p>The object is composed of several nodes, defining the outcome and covariates, as well as other information that might be useful (e.g., weights to assign to specific covariates, and subject IDs to effectively deal with repeated measures).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">task<span class="op">$</span>nodes</code></pre></div>
<pre><code>## $covariates
## [1] "apgar1"   "apgar5"   "parity"   "gagebrth" "mage"     "meducyrs"
## [7] "sexn"    
## 
## $outcome
## [1] "haz"
## 
## $id
## NULL
## 
## $weights
## NULL</code></pre>
<p>As discussed above, the design of <code>sl3</code> allows for screening and learning algorithms to be <em>chained</em> to form <em>pipelines</em>. Let’s take a look at how we might set up a pipeline.</p>
<p>To begin, we’ll need to specify a screening algorithm to use in covariate selection as well as a learning algorithm. We set up a simple elastic net screener (accessed directly from the wide selection already available in the <code>Super Learner</code> package) and a GLM learner, provided directly with the <code>sl3</code> package.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">slscreener &lt;-<span class="st"> </span>Lrnr_pkg_SuperLearner_screener<span class="op">$</span><span class="kw">new</span>(<span class="st">"screen.glmnet"</span>)
glm_learner &lt;-<span class="st"> </span>Lrnr_glm<span class="op">$</span><span class="kw">new</span>()</code></pre></div>
<p>Having specified a screener and a learner, we are now ready to set up a pipeline, combining these algorithms in the manner outlined by the Super Learner algorithm.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">screen_and_glm &lt;-<span class="st"> </span>Pipeline<span class="op">$</span><span class="kw">new</span>(slscreener, glm_learner)
SL.glmnet_learner &lt;-<span class="st"> </span>Lrnr_pkg_SuperLearner<span class="op">$</span><span class="kw">new</span>(<span class="dt">SL_wrapper =</span> <span class="st">"SL.glmnet"</span>)
sg_fit &lt;-<span class="st"> </span>screen_and_glm<span class="op">$</span><span class="kw">train</span>(task)</code></pre></div>
<pre><code>## Loading required package: glmnet</code></pre>
<pre><code>## Loading required package: Matrix</code></pre>
<pre><code>## Loading required package: foreach</code></pre>
<pre><code>## Loaded glmnet 2.0-13</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">print</span>(sg_fit)</code></pre></div>
<pre><code>## $Lrnr_pkg_SuperLearner_screener_screen.glmnet
## [1] "Lrnr_pkg_SuperLearner_screener_screen.glmnet"
## $selected
## [1] "apgar1"   "parity"   "gagebrth" "mage"     "meducyrs" "sexn"    
## 
## 
## $Lrnr_glm
## [1] "Lrnr_glm"
## 
## Call:  stats::glm(formula = Y ~ ., family = family, data = task$X, weights = task$weights)
## 
## Coefficients:
## (Intercept)       apgar1       parity     gagebrth         mage  
##   -5.540529     0.013162    -0.081139     0.019678     0.017596  
##    meducyrs         sexn  
##   -0.004884    -0.079807  
## 
## Degrees of Freedom: 1440 Total (i.e. Null);  1434 Residual
## Null Deviance:       2353 
## Residual Deviance: 2280  AIC: 4766</code></pre>
<pre><code>## $learner_fits
## $learner_fits$Lrnr_pkg_SuperLearner_screener_screen.glmnet
## [1] "Lrnr_pkg_SuperLearner_screener_screen.glmnet"
## $selected
## [1] "apgar1"   "parity"   "gagebrth" "mage"     "meducyrs" "sexn"    
## 
## 
## $learner_fits$Lrnr_glm
## [1] "Lrnr_glm"
## 
## Call:  stats::glm(formula = Y ~ ., family = family, data = task$X, weights = task$weights)
## 
## Coefficients:
## (Intercept)       apgar1       parity     gagebrth         mage  
##   -5.540529     0.013162    -0.081139     0.019678     0.017596  
##    meducyrs         sexn  
##   -0.004884    -0.079807  
## 
## Degrees of Freedom: 1440 Total (i.e. Null);  1434 Residual
## Null Deviance:       2353 
## Residual Deviance: 2280  AIC: 4766</code></pre>
<p>The output from the <code>Pipeline</code> object provides us with a few key pieces of information:</p>
<ol style="list-style-type: decimal">
<li>The screening algorithm (<code>glmnet</code> in this case), or library of screeners, as well as the variables selected by the screener.</li>
<li>The learning algorithm (<code>glm</code> in this case), or library of learners, as well as the standard output produced by the learning algorithm being invoked, when fit with the covariates selected by the screener.</li>
</ol>
<p>What happens if we want to perform model stacking (whether with discrete or ensemble Super Learner)?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">learner_stack &lt;-<span class="st"> </span>Stack<span class="op">$</span><span class="kw">new</span>(SL.glmnet_learner, glm_learner, screen_and_glm)
stack_fit &lt;-<span class="st"> </span>learner_stack<span class="op">$</span><span class="kw">train</span>(task)</code></pre></div>
<p>As is clear from the above, we can create a <em>stack</em> of learners (of a class named eponymously, and sub-classing the <code>Lrnr_base</code> class) with a rather simple call; moreover, stacks may include not only learners themselves but also pipelines. (Recall that we created a pipeline above by combining both a screener and a learner). Notably, since cross-validation is applied as part of model stacking in the Super Learner algorithm, such a stacking design means that pipelines themselves may be cross-validated. What is more, from the calls immediately above, we note that creating stacked regressions and training such models is an essentially trivial process.</p>
<p>We can examine the results of our stacked regression model by looking at the predictions (made on the training data, though we could very easily pass in new data as well):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">preds &lt;-<span class="st"> </span>stack_fit<span class="op">$</span><span class="kw">predict</span>()
<span class="kw">head</span>(preds)</code></pre></div>
<pre><code>##    Lrnr_pkg_SuperLearner_SL.glmnet   Lrnr_glm
## 1:                      0.35423935 0.36298498
## 2:                      0.35423935 0.36298498
## 3:                      0.24671276 0.25993072
## 4:                      0.24671276 0.25993072
## 5:                      0.24671276 0.25993072
## 6:                      0.03188413 0.05680264
##    Lrnr_pkg_SuperLearner_screener_screen.glmnet___Lrnr_glm
## 1:                                              0.36228209
## 2:                                              0.36228209
## 3:                                              0.25870995
## 4:                                              0.25870995
## 5:                                              0.25870995
## 6:                                              0.05600958</code></pre>
<p>In the above, we obtain prediction for the first few observations in the data set, for each of the learners that compose the stacked regression model.</p>
<p>We can create a stacked regression model that incorporates cross-validation by simply using the built-in <code>Lrnr_cv</code> class, and we can train the cross-validated stacked regression model using its <code>train</code> method:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">cv_stack &lt;-<span class="st"> </span>Lrnr_cv<span class="op">$</span><span class="kw">new</span>(learner_stack)
cv_fit &lt;-<span class="st"> </span>cv_stack<span class="op">$</span><span class="kw">train</span>(task)</code></pre></div>
<p>Note that in the above we create our cross-validated stacked regression model by invoking a new <code>Lrnr_cv</code> and passing in the model stack that we built above. This means that our <code>Lrnr_cv</code> is built on top of our previous call to <code>Stack$new()</code>.</p>
<p>To fit a meta-learner on the cross-validated predictions, we need only create a new pipeline – one that includes the appropriate model stack and specifies the meta-learning algorithm to be used. Here, we create a meta-learner from a GLM (by specifying <code>glm_learner</code>) and provide the cross-validated stacked regression model as the library over which the meta-learner is to operate.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">glm_stack &lt;-<span class="st"> </span>Pipeline<span class="op">$</span><span class="kw">new</span>(cv_stack, glm_learner)
ml_fit &lt;-<span class="st"> </span>glm_stack<span class="op">$</span><span class="kw">train</span>(task)</code></pre></div>
<p>Above, we build the meta-learner by using a pipeline and then simply call the <code>train</code> method from the meta-learner, passing in the data object (the <code>Task</code> we created at the very beginning of this exercise).</p>
<p>Let us take a look at the object created from training the GLM meta-learner:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">print</span>(ml_fit)</code></pre></div>
<pre><code>## $CV_Stack
## [1] "Lrnr_cv"
## [1] "Lrnr_pkg_SuperLearner_SL.glmnet"
## [1] "Lrnr_glm"
## [1] "Lrnr_pkg_SuperLearner_screener_screen.glmnet"
## [1] "Lrnr_glm"
## 
## $Lrnr_glm
## [1] "Lrnr_glm"
## 
## Call:  stats::glm(formula = Y ~ ., family = family, data = task$X, weights = task$weights)
## 
## Coefficients:
##                                             (Intercept)  
##                                                 0.01958  
##                         Lrnr_pkg_SuperLearner_SL.glmnet  
##                                                 0.96532  
##                                                Lrnr_glm  
##                                                 1.18916  
## Lrnr_pkg_SuperLearner_screener_screen.glmnet___Lrnr_glm  
##                                                -1.27656  
## 
## Degrees of Freedom: 1440 Total (i.e. Null);  1437 Residual
## Null Deviance:       2353 
## Residual Deviance: 2307  AIC: 4778</code></pre>
<pre><code>## $learner_fits
## $learner_fits$CV_Stack
## [1] "Lrnr_cv"
## [1] "Lrnr_pkg_SuperLearner_SL.glmnet"
## [1] "Lrnr_glm"
## [1] "Lrnr_pkg_SuperLearner_screener_screen.glmnet"
## [1] "Lrnr_glm"
## 
## $learner_fits$Lrnr_glm
## [1] "Lrnr_glm"
## 
## Call:  stats::glm(formula = Y ~ ., family = family, data = task$X, weights = task$weights)
## 
## Coefficients:
##                                             (Intercept)  
##                                                 0.01958  
##                         Lrnr_pkg_SuperLearner_SL.glmnet  
##                                                 0.96532  
##                                                Lrnr_glm  
##                                                 1.18916  
## Lrnr_pkg_SuperLearner_screener_screen.glmnet___Lrnr_glm  
##                                                -1.27656  
## 
## Degrees of Freedom: 1440 Total (i.e. Null);  1437 Residual
## Null Deviance:       2353 
## Residual Deviance: 2307  AIC: 4778</code></pre>
<p>And the first few prediction from the model:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ml_fit_preds &lt;-<span class="st"> </span>ml_fit<span class="op">$</span><span class="kw">predict</span>()
<span class="kw">head</span>(ml_fit_preds)</code></pre></div>
<pre><code>##          1          2          3          4          5          6 
## 0.32800411 0.31457899 0.20850463 0.29322737 0.29322737 0.00831161</code></pre>
<p>Finally, we can build a proper Super Learner in a single call by providing the library of learners as well as the meta-learner. For simplicity, we will use the same set of learners and meta-learning algorithm as we did before, which means that we are merely building a GLM-based Super Learner (of class <code>Lrnr_sl</code>), with learning algorithms including an elastic net, GLM, and GLM with elastic net screener. Below, we train the Super Learner on the data simply by invoking the <code>train</code> method:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># convenience learner combining all this</span>
sl &lt;-<span class="st"> </span>Lrnr_sl<span class="op">$</span><span class="kw">new</span>(<span class="dt">learners =</span> <span class="kw">list</span>(SL.glmnet_learner, glm_learner,
                                  screen_and_glm),
                  <span class="dt">metalearner =</span> glm_learner)
sl_fit &lt;-<span class="st"> </span>sl<span class="op">$</span><span class="kw">train</span>(task)</code></pre></div>
<p>The <code>Lrnr_sl</code> object we create includes a great deal of information (thus we regrain from examining it in detail).</p>
<hr>
</div>
<div id="session-information" class="section level2">
<h2 class="hasAnchor">
<a href="#session-information" class="anchor"></a>Session Information</h2>
<pre><code>## R version 3.4.2 (2017-09-28)
## Platform: x86_64-apple-darwin16.7.0 (64-bit)
## Running under: macOS Sierra 10.12.6
## 
## Matrix products: default
## BLAS: /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib
## LAPACK: /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libLAPACK.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] glmnet_2.0-13            foreach_1.4.3           
##  [3] Matrix_1.2-11            bindrcpp_0.2            
##  [5] SuperLearner_2.0-23-9000 nnls_1.4                
##  [7] origami_0.8.0            sl3_0.2.2               
##  [9] dplyr_0.7.4              data.table_1.10.4       
## 
## loaded via a namespace (and not attached):
##  [1] h2o_3.14.0.3        listenv_0.6.0       lattice_0.20-35    
##  [4] colorspace_1.3-2    htmltools_0.3.6     yaml_2.1.14        
##  [7] rlang_0.1.2.9000    glue_1.1.1          xgboost_0.6-4      
## [10] speedglm_0.3-2      uuid_0.1-2          bindr_0.1          
## [13] stringr_1.2.0.9000  visNetwork_2.0.1    future_1.6.1       
## [16] htmlwidgets_0.9     codetools_0.2-15    evaluate_0.10.1    
## [19] memoise_1.1.0       knitr_1.17          parallel_3.4.2     
## [22] rstackdeque_1.1.1   Rcpp_0.12.13        delayed_0.2.1      
## [25] backports_1.1.1     checkmate_1.8.4     jsonlite_1.5       
## [28] truncnorm_1.0-7     abind_1.4-5         digest_0.6.12      
## [31] stringi_1.1.5       BBmisc_1.11         rprojroot_1.2      
## [34] grid_3.4.2          tools_3.4.2         bitops_1.0-6       
## [37] magrittr_1.5        RCurl_1.95-4.8      Rsolnp_1.16        
## [40] tibble_1.3.4        randomForest_4.6-12 pkgconfig_2.0.1    
## [43] MASS_7.3-47         assertthat_0.2.0    rmarkdown_1.6      
## [46] iterators_1.0.8     R6_2.2.2            globals_0.10.2     
## [49] igraph_1.1.2        compiler_3.4.2</code></pre>
<hr>
</div>
<div id="references" class="section level2 unnumbered">
<h2 class="hasAnchor">
<a href="#references" class="anchor"></a>References</h2>
<div id="refs" class="references">
<div id="ref-breiman1996stacked">
<p>Breiman, Leo. 1996. “Stacked Regressions.” <em>Machine Learning</em> 24 (1). Springer: 49–64.</p>
</div>
<div id="ref-vdl2007super">
<p>van der Laan, Mark J., Eric C. Polley, and Alan E. Hubbard. 2007. “Super Learner.” <em>Statistical Applications in Genetics and Molecular Biology</em> 6 (1).</p>
</div>
</div>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
        <div id="tocnav">
      <h2 class="hasAnchor">
<a href="#tocnav" class="anchor"></a>Contents</h2>
      <ul class="nav nav-pills nav-stacked">
<li><a href="#introduction">Introduction</a></li>
      <li><a href="#a-whirlwind-tour">A Whirlwind Tour</a></li>
      <li><a href="#session-information">Session Information</a></li>
      <li><a href="#references">References</a></li>
      </ul>
</div>
      </div>

</div>


      <footer><div class="copyright">
  <p>Developed by Jeremy Coyle, Nima Hejazi, Oleg Sofrygin, Ivana Malenica.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://hadley.github.io/pkgdown/">pkgdown</a>.</p>
</div>

      </footer>
</div>

  </body>
</html>
